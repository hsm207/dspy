# docker compose for the devcontainer and the mistral llm
version: '3.8'
services:
  devcontainer:
    image: mcr.microsoft.com/devcontainers/python:1-3.12-bullseye
    command: sleep infinity
    volumes:
      - ../..:/workspaces:cached
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
  llm:
    image: ghcr.io/huggingface/text-generation-inference:1.4
    command: ["--model-id", "${MODEL}", "--num-shard", "${NUM_SHARD}"]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${TOKEN}
      - MAX_INPUT_LENGTH=2048
      - MAX_TOTAL_TOKENS=4096
    volumes:
      - ${VOLUME}:/data
    ports:
      - 8080:80
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: '1g'